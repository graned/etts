# üó£Ô∏è ETTS ‚Äì Echora Text-to-Speech Engine

**ETTS** is a custom deep learning pipeline for generating expressive speech using PyTorch.  
It is designed to support multilingual, multi-speaker voice synthesis ‚Äî including future capabilities like prompt-to-voice and voice cloning.

---

## Project Structure

```
etts
‚îú‚îÄ‚îÄ data
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ etts_dataloader.py
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ etts_dataset.py
‚îÇ¬†¬†
‚îú‚îÄ‚îÄ models
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ etts_model.py
‚îú‚îÄ‚îÄ pyrightconfig.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ test
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dummy_dictionaries
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ phoneme_dict.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dummy_samples
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ embedding_extractor.test.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ etts_dataloader.test.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ etts_dataset.test.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ etts_model.test.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ manifest_builder.test.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ mel_extractor.test.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ dummy_manifest.json
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ phoneme_dictionary.test.py
‚îú‚îÄ‚îÄ train
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ checkpoints
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dictionaries
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ phoneme_dict.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ manifests
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ etts_manifest.json
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ samples
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ en_us
‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ sample1
‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ audio.mp3
‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ transcript.txt
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train_etts.py
‚îî‚îÄ‚îÄ utils
    ‚îú‚îÄ‚îÄ embedding_extractor.py
    ‚îú‚îÄ‚îÄ manifest_builder.py
    ‚îú‚îÄ‚îÄ mel_extractor.py
    ‚îî‚îÄ‚îÄ phoneme_dictionary.py


```

---

## üí° Conceptual Overview

Human speech is the result of three critical components:

1. **Phonemes** ‚Äî The basic units of sound (e.g., "k", "aa", "t" in "cat").
2. **Speaker Embedding** ‚Äî A learned vector that captures a person's unique voice characteristics.
3. **Mel Spectrogram** ‚Äî A time-frequency representation of audio, showing how pitch and energy evolve over time.

These pieces are combined in a neural network to generate speech from text input.

---

## üß† How It Works

     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ  Text      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Phonemizer  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ Phoneme IDs‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                       ‚îÇ  Speaker Embedding (SE) ‚îÇ
                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
                                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                     ‚îÇ       ETTS Model           ‚îÇ
                                     ‚îÇ (Phonemes + SE ‚Üí Mel Spec) ‚îÇ
                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
                               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                               ‚îÇ      Vocoder (HiFi-GAN or similar)     ‚îÇ
                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
                                              üîä Final Audio Output

---

## üõ†Ô∏è Components

| Module         | Description                                                                                      |
| -------------- | ------------------------------------------------------------------------------------------------ |
| `model.py`     | PyTorch model that takes phoneme sequences and speaker embeddings, and predicts mel spectrograms |
| `train.py`     | Basic training loop using synthetic or real data                                                 |
| `dataset.py`   | Custom Dataset class that loads `.npz` files with phonemes, embeddings, and mel spectrograms     |
| `utils.py`     | Helper functions for phonemizing, visualizing, and processing audio                              |
| `scripts/`     | Tools for preprocessing audio, generating dummy data, and more                                   |
| `checkpoints/` | Where trained model weights will be saved                                                        |

---

## üì¶ Dataset Format

Training data is stored in `.npz` files with the following keys:

- `phonemes`: (T,) ‚Äî array of phoneme IDs
- `embedding`: (256,) ‚Äî speaker embedding
- `mel`: (T, 80) ‚Äî mel spectrogram with 80 bins

---

## üîÅ Training Loop (Simplified)

1. Load phoneme IDs
2. Load speaker embedding
3. Load ground-truth mel spectrogram
4. Predict mel spectrogram from model
5. Compute loss (MSE)
6. Backpropagation and optimizer step

---

## üìà Visual: Mel Spectrogram

Here's a mel spectrogram of the word **"hello"**:

![mel spectrogram example](https://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Spectrogram-19thC.png/800px-Spectrogram-19thC.png)

> The horizontal axis is time; the vertical is frequency; color intensity shows energy.

---

## üß™ Goals and Future Ideas

- [x] Modular pipeline for phoneme + embedding ‚Üí mel
- [ ] Add HiFi-GAN vocoder for final waveform synthesis
- [ ] Voice cloning from reference audio
- [ ] Prompt-to-voice generation using descriptions (e.g., "older female, calm, storyteller")
- [ ] Multi-language support via `phonemizer` and language-specific tokenizers
- [ ] TinyML export or serverless inference (low-latency TTS)

---

## ‚öôÔ∏è Installation

```bash
pip install -r requirements.txt

```
