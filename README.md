# üó£Ô∏è ETTS ‚Äì Echora Text-to-Speech Engine

**ETTS** is a custom deep learning pipeline for generating expressive speech using PyTorch.  
It is designed to support multilingual, multi-speaker voice synthesis ‚Äî including future capabilities like prompt-to-voice and voice cloning.

---

## Project Structure

```
etts/
‚îú‚îÄ‚îÄ etts/                    ‚Üê Python package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model.py             ‚Üê Your PyTorch model definition
‚îÇ   ‚îú‚îÄ‚îÄ train.py             ‚Üê Training loop
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py           ‚Üê Custom dataset loader
‚îÇ   ‚îú‚îÄ‚îÄ utils.py             ‚Üê Helpers (phonemizer, spectrograms, etc.)
‚îÇ
‚îú‚îÄ‚îÄ scripts/                 ‚Üê Optional tools for audio/text preprocessing
‚îÇ   ‚îî‚îÄ‚îÄ create_dummy_data.py
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/             ‚Üê Saved model weights
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ main.py                  ‚Üê CLI entrypoint (e.g., `python main.py --infer "Hello"`)
```

---

## üí° Conceptual Overview

Human speech is the result of three critical components:

1. **Phonemes** ‚Äî The basic units of sound (e.g., "k", "aa", "t" in "cat").
2. **Speaker Embedding** ‚Äî A learned vector that captures a person's unique voice characteristics.
3. **Mel Spectrogram** ‚Äî A time-frequency representation of audio, showing how pitch and energy evolve over time.

These pieces are combined in a neural network to generate speech from text input.

---

## üß† How It Works

     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ  Text      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Phonemizer  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ Phoneme IDs‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                       ‚îÇ  Speaker Embedding (SE) ‚îÇ
                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
                                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                     ‚îÇ       ETTS Model           ‚îÇ
                                     ‚îÇ (Phonemes + SE ‚Üí Mel Spec) ‚îÇ
                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
                               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                               ‚îÇ      Vocoder (HiFi-GAN or similar)     ‚îÇ
                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
                                                        ‚ñº
                                              üîä Final Audio Output

---

## üõ†Ô∏è Components

| Module | Description |
|--------|-------------|
| `model.py` | PyTorch model that takes phoneme sequences and speaker embeddings, and predicts mel spectrograms |
| `train.py` | Basic training loop using synthetic or real data |
| `dataset.py` | Custom Dataset class that loads `.npz` files with phonemes, embeddings, and mel spectrograms |
| `utils.py` | Helper functions for phonemizing, visualizing, and processing audio |
| `scripts/` | Tools for preprocessing audio, generating dummy data, and more |
| `checkpoints/` | Where trained model weights will be saved |

---

## üì¶ Dataset Format

Training data is stored in `.npz` files with the following keys:

- `phonemes`: (T,) ‚Äî array of phoneme IDs
- `embedding`: (256,) ‚Äî speaker embedding
- `mel`: (T, 80) ‚Äî mel spectrogram with 80 bins

---

## üîÅ Training Loop (Simplified)

1. Load phoneme IDs
2. Load speaker embedding
3. Load ground-truth mel spectrogram
4. Predict mel spectrogram from model
5. Compute loss (MSE)
6. Backpropagation and optimizer step

---

## üìà Visual: Mel Spectrogram

Here's a mel spectrogram of the word **"hello"**:

![mel spectrogram example](https://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Spectrogram-19thC.png/800px-Spectrogram-19thC.png)

> The horizontal axis is time; the vertical is frequency; color intensity shows energy.

---

## üß™ Goals and Future Ideas

- [x] Modular pipeline for phoneme + embedding ‚Üí mel
- [ ] Add HiFi-GAN vocoder for final waveform synthesis
- [ ] Voice cloning from reference audio
- [ ] Prompt-to-voice generation using descriptions (e.g., "older female, calm, storyteller")
- [ ] Multi-language support via `phonemizer` and language-specific tokenizers
- [ ] TinyML export or serverless inference (low-latency TTS)

---

## ‚öôÔ∏è Installation

```bash
pip install -r requirements.txt

